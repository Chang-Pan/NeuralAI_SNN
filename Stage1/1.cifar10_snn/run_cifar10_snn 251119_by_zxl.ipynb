{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1b9769a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38838a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.1+cu128\n",
      "CUDA available: True\n",
      "Current device: NVIDIA GeForce RTX 3090\n",
      "Device count: 1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import time\n",
    "from spikingjelly.activation_based import neuron, functional, layer\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Current device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Device count: {torch.cuda.device_count()}\") # 应该输出 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "002a6234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 实验设置 ---\n",
      "设备 (DEVICE): cuda:0\n",
      "仿真时长 (T): 8\n",
      "批大小 (BATCH_SIZE): 64\n",
      "训练轮数 (EPOCHS): 10\n",
      "------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------\n",
    "# 1. 定义超参数和设置\n",
    "# ----------------------------------------\n",
    "\n",
    "T = 8             # 仿真总时长 (SNN 的关键参数)\n",
    "BATCH_SIZE = 64   # 批处理大小\n",
    "EPOCHS = 10       # 训练轮数 (为快速演示，设置较小)\n",
    "LR = 1e-3         # 学习率\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"--- 实验设置 ---\")\n",
    "print(f\"设备 (DEVICE): {DEVICE}\")\n",
    "print(f\"仿真时长 (T): {T}\")\n",
    "print(f\"批大小 (BATCH_SIZE): {BATCH_SIZE}\")\n",
    "print(f\"训练轮数 (EPOCHS): {EPOCHS}\")\n",
    "print(f\"------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b391e614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在加载 CIFAR10 数据集...\n",
      "数据集加载完毕。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------\n",
    "# 2. 加载和预处理 CIFAR10 数据集\n",
    "# ----------------------------------------\n",
    "print(\"正在加载 CIFAR10 数据集...\")\n",
    "# CIFAR10 图像的均值和标准差 (用于归一化)\n",
    "cifar_mean = (0.4914, 0.4822, 0.4465)\n",
    "cifar_std = (0.2023, 0.1994, 0.2010)\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(), # 简单数据增强：随机翻转\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(cifar_mean, cifar_std)\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(cifar_mean, cifar_std)\n",
    "])\n",
    "\n",
    "# 加载数据\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "print(\"数据集加载完毕。\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86696b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------\n",
    "# 3. 定义基础的卷积 SNN 模型\n",
    "# ----------------------------------------\n",
    "# 使用 nn.Sequential 快速搭建一个简单的 CNN 结构\n",
    "# 关键在于在激活函数的位置换上 SNN 的脉冲神经元\n",
    "\n",
    "class BasicCSNN(nn.Module):\n",
    "    def __init__(self, T):\n",
    "        super().__init__()\n",
    "        self.T = T  # 保存仿真时长\n",
    "\n",
    "        # 定义网络结构\n",
    "        # 结构：[卷积 -> 脉冲 -> 池化] x 2 -> [展平 -> 全连接 -> 脉冲] -> [全连接]\n",
    "        self.net = nn.Sequential(\n",
    "            # 块 1\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            # --- 核心：使用 LIF 神经元 ---\n",
    "            # 激活驱动:LIFNode 在前向传播时模拟 LIF 神经元动力学，在反向传播时，SpikingJelly 会自动使用“替代梯度”进行计算。\n",
    "            neuron.LIFNode(),\n",
    "            nn.MaxPool2d(2),  # 32x32 -> 16x16\n",
    "\n",
    "            # 块 2\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            neuron.LIFNode(),\n",
    "            nn.MaxPool2d(2),  # 16x16 -> 8x8\n",
    "\n",
    "            # 展平\n",
    "            nn.Flatten(),\n",
    "\n",
    "            # 全连接层 1\n",
    "            nn.Linear(64 * 8 * 8, 128), # 64 * 8 * 8 = 4096\n",
    "            neuron.LIFNode(),\n",
    "\n",
    "            # 输出层 (全连接层 2)\n",
    "            # 输出层通常不使用脉冲神经元，而是直接输出膜电位或累积电流\n",
    "            # 这样可以方便地与交叉熵损失配合使用\n",
    "            nn.Linear(128, 10) # 10个类别\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # --- SNN 算法思路的核心 ---\n",
    "        # SNN 神经元是有状态的（例如膜电位 V），在处理一个新样本前必须重置\n",
    "        # 1. 重置网络中所有神经元的状态\n",
    "        functional.reset_net(self)\n",
    "\n",
    "        # 准备一个列表来收集 T 个时间步的输出\n",
    "        # (T, N, C)，T=时间步, N=BatchSize, C=类别数\n",
    "        outputs_over_time = []\n",
    "\n",
    "        # 2. SNN 的时间步循环\n",
    "        # 对于静态图像 (如CIFAR10)，我们在 T 个时间步内输入 *相同* 的图像 x\n",
    "        # 神经元会在这 T 步内不断累积输入并发放脉冲\n",
    "        for t in range(self.T):\n",
    "            # 运行一步前向传播\n",
    "            out_t = self.net(x)\n",
    "            outputs_over_time.append(out_t)\n",
    "\n",
    "        # 3. 聚合 T 个时间步的输出\n",
    "        # (T, N, 10) -> (T, N, 10)\n",
    "        outputs_stack = torch.stack(outputs_over_time)\n",
    "        \n",
    "        # 4. 解码：计算 T 步内的平均输出\n",
    "        # (T, N, 10) -> (N, 10)\n",
    "        # 我们取所有时间步输出的平均值，作为最终的分类 \"logits\"\n",
    "        # 这是一种常见的 SNN 解码方式（Rate Coding / Mean Output）\n",
    "        return outputs_stack.mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8cf29bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------\n",
    "# 4. 初始化模型、损失函数和优化器\n",
    "# ----------------------------------------\n",
    "model = BasicCSNN(T=T).to(DEVICE)\n",
    "# print(\"模型结构:\\n\", model) # (取消注释以查看模型)\n",
    "\n",
    "# 使用标准的交叉熵损失函数\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 使用 Adam 优化器\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5d6e739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------\n",
    "# 5. 编写训练和评估循环\n",
    "# ----------------------------------------\n",
    "\n",
    "# --- 训练函数 (Train Loop) ---\n",
    "def train_epoch(epoch):\n",
    "    model.train()  # 设置为训练模式\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "\n",
    "        # 1. 梯度清零\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 2. 前向传播\n",
    "        #    模型内部会自动处理 T 个时间步的循环\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # 3. 计算损失\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # 4. 反向传播 (核心)\n",
    "        #    PyTorch 在这里调用 .backward()\n",
    "        #    SpikingJelly 的 LIFNode 会自动拦截梯度计算，\n",
    "        #    并用“替代梯度”替换掉不可导的脉冲激活函数梯度。\n",
    "        loss.backward()\n",
    "\n",
    "        # 5. 更新参数\n",
    "        optimizer.step()\n",
    "\n",
    "        # 统计损失和准确率\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        if (batch_idx + 1) % 100 == 0:\n",
    "            print(f\"  [Epoch {epoch+1}/{EPOCHS}, Batch {batch_idx+1}/{len(train_loader)}] \"\n",
    "                  f\"Loss: {total_loss / (batch_idx + 1):.4f} | \"\n",
    "                  f\"Acc: {100. * correct / total:.2f}%\")\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"Epoch {epoch+1} 训练完成。用时: {end_time - start_time:.2f}秒\")\n",
    "    print(f\"  训练集平均 Loss: {total_loss / len(train_loader):.4f}, \"\n",
    "          f\"训练集准确率: {100. * correct / total:.2f}%\")\n",
    "\n",
    "# --- 评估函数 (Eval Loop) ---\n",
    "def test_epoch(epoch):\n",
    "    model.eval()  # 设置为评估模式\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # 评估时不需要计算梯度\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "\n",
    "            # 前向传播\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # 计算损失\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # 统计准确率\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    test_acc = 100. * correct / total\n",
    "    test_loss = total_loss / len(test_loader)\n",
    "    print(f\"--- Epoch {epoch+1} 测试结果 ---\")\n",
    "    print(f\"  测试集 Loss: {test_loss:.4f}, 测试集准确率 (Acc): {test_acc:.2f}%\")\n",
    "    print(f\"--------------------------\\n\")\n",
    "    return test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f8600fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 开始训练 ===\n",
      "  [Epoch 1/10, Batch 100/782] Loss: 2.1331 | Acc: 21.31%\n",
      "  [Epoch 1/10, Batch 200/782] Loss: 1.9418 | Acc: 29.46%\n",
      "  [Epoch 1/10, Batch 300/782] Loss: 1.8233 | Acc: 33.80%\n",
      "  [Epoch 1/10, Batch 400/782] Loss: 1.7321 | Acc: 37.36%\n",
      "  [Epoch 1/10, Batch 500/782] Loss: 1.6718 | Acc: 39.66%\n",
      "  [Epoch 1/10, Batch 600/782] Loss: 1.6236 | Acc: 41.30%\n",
      "  [Epoch 1/10, Batch 700/782] Loss: 1.5816 | Acc: 42.83%\n",
      "Epoch 1 训练完成。用时: 16.02秒\n",
      "  训练集平均 Loss: 1.5531, 训练集准确率: 43.86%\n",
      "--- Epoch 1 测试结果 ---\n",
      "  测试集 Loss: 1.3140, 测试集准确率 (Acc): 52.39%\n",
      "--------------------------\n",
      "\n",
      "  [Epoch 2/10, Batch 100/782] Loss: 1.2630 | Acc: 54.64%\n",
      "  [Epoch 2/10, Batch 200/782] Loss: 1.2340 | Acc: 56.11%\n",
      "  [Epoch 2/10, Batch 300/782] Loss: 1.2259 | Acc: 56.34%\n",
      "  [Epoch 2/10, Batch 400/782] Loss: 1.2158 | Acc: 56.75%\n",
      "  [Epoch 2/10, Batch 500/782] Loss: 1.2041 | Acc: 57.18%\n",
      "  [Epoch 2/10, Batch 600/782] Loss: 1.1957 | Acc: 57.41%\n",
      "  [Epoch 2/10, Batch 700/782] Loss: 1.1849 | Acc: 57.78%\n",
      "Epoch 2 训练完成。用时: 13.68秒\n",
      "  训练集平均 Loss: 1.1764, 训练集准确率: 58.11%\n",
      "--- Epoch 2 测试结果 ---\n",
      "  测试集 Loss: 1.0779, 测试集准确率 (Acc): 62.12%\n",
      "--------------------------\n",
      "\n",
      "  [Epoch 3/10, Batch 100/782] Loss: 1.0574 | Acc: 62.20%\n",
      "  [Epoch 3/10, Batch 200/782] Loss: 1.0505 | Acc: 62.49%\n",
      "  [Epoch 3/10, Batch 300/782] Loss: 1.0460 | Acc: 62.82%\n",
      "  [Epoch 3/10, Batch 400/782] Loss: 1.0342 | Acc: 63.37%\n",
      "  [Epoch 3/10, Batch 500/782] Loss: 1.0273 | Acc: 63.56%\n",
      "  [Epoch 3/10, Batch 600/782] Loss: 1.0200 | Acc: 63.86%\n",
      "  [Epoch 3/10, Batch 700/782] Loss: 1.0177 | Acc: 64.01%\n",
      "Epoch 3 训练完成。用时: 13.51秒\n",
      "  训练集平均 Loss: 1.0119, 训练集准确率: 64.18%\n",
      "--- Epoch 3 测试结果 ---\n",
      "  测试集 Loss: 0.9800, 测试集准确率 (Acc): 65.30%\n",
      "--------------------------\n",
      "\n",
      "  [Epoch 4/10, Batch 100/782] Loss: 0.9138 | Acc: 68.11%\n",
      "  [Epoch 4/10, Batch 200/782] Loss: 0.9148 | Acc: 68.04%\n",
      "  [Epoch 4/10, Batch 300/782] Loss: 0.9169 | Acc: 67.94%\n",
      "  [Epoch 4/10, Batch 400/782] Loss: 0.9177 | Acc: 67.81%\n",
      "  [Epoch 4/10, Batch 500/782] Loss: 0.9134 | Acc: 67.97%\n",
      "  [Epoch 4/10, Batch 600/782] Loss: 0.9130 | Acc: 67.87%\n",
      "  [Epoch 4/10, Batch 700/782] Loss: 0.9110 | Acc: 67.92%\n",
      "Epoch 4 训练完成。用时: 14.15秒\n",
      "  训练集平均 Loss: 0.9056, 训练集准确率: 68.18%\n",
      "--- Epoch 4 测试结果 ---\n",
      "  测试集 Loss: 0.9476, 测试集准确率 (Acc): 66.04%\n",
      "--------------------------\n",
      "\n",
      "  [Epoch 5/10, Batch 100/782] Loss: 0.8443 | Acc: 70.33%\n",
      "  [Epoch 5/10, Batch 200/782] Loss: 0.8399 | Acc: 70.70%\n",
      "  [Epoch 5/10, Batch 300/782] Loss: 0.8381 | Acc: 70.77%\n",
      "  [Epoch 5/10, Batch 400/782] Loss: 0.8335 | Acc: 70.88%\n",
      "  [Epoch 5/10, Batch 500/782] Loss: 0.8356 | Acc: 70.69%\n",
      "  [Epoch 5/10, Batch 600/782] Loss: 0.8341 | Acc: 70.69%\n",
      "  [Epoch 5/10, Batch 700/782] Loss: 0.8352 | Acc: 70.70%\n",
      "Epoch 5 训练完成。用时: 15.92秒\n",
      "  训练集平均 Loss: 0.8318, 训练集准确率: 70.86%\n",
      "--- Epoch 5 测试结果 ---\n",
      "  测试集 Loss: 0.8941, 测试集准确率 (Acc): 68.46%\n",
      "--------------------------\n",
      "\n",
      "  [Epoch 6/10, Batch 100/782] Loss: 0.7582 | Acc: 72.98%\n",
      "  [Epoch 6/10, Batch 200/782] Loss: 0.7675 | Acc: 73.17%\n",
      "  [Epoch 6/10, Batch 300/782] Loss: 0.7673 | Acc: 73.13%\n",
      "  [Epoch 6/10, Batch 400/782] Loss: 0.7690 | Acc: 73.12%\n",
      "  [Epoch 6/10, Batch 500/782] Loss: 0.7691 | Acc: 73.07%\n",
      "  [Epoch 6/10, Batch 600/782] Loss: 0.7708 | Acc: 73.08%\n",
      "  [Epoch 6/10, Batch 700/782] Loss: 0.7702 | Acc: 73.06%\n",
      "Epoch 6 训练完成。用时: 14.88秒\n",
      "  训练集平均 Loss: 0.7702, 训练集准确率: 73.04%\n",
      "--- Epoch 6 测试结果 ---\n",
      "  测试集 Loss: 0.9005, 测试集准确率 (Acc): 68.61%\n",
      "--------------------------\n",
      "\n",
      "  [Epoch 7/10, Batch 100/782] Loss: 0.7230 | Acc: 75.27%\n",
      "  [Epoch 7/10, Batch 200/782] Loss: 0.7167 | Acc: 75.26%\n",
      "  [Epoch 7/10, Batch 300/782] Loss: 0.7127 | Acc: 75.52%\n",
      "  [Epoch 7/10, Batch 400/782] Loss: 0.7158 | Acc: 75.37%\n",
      "  [Epoch 7/10, Batch 500/782] Loss: 0.7144 | Acc: 75.27%\n",
      "  [Epoch 7/10, Batch 600/782] Loss: 0.7176 | Acc: 75.10%\n",
      "  [Epoch 7/10, Batch 700/782] Loss: 0.7160 | Acc: 75.10%\n",
      "Epoch 7 训练完成。用时: 14.24秒\n",
      "  训练集平均 Loss: 0.7172, 训练集准确率: 75.04%\n",
      "--- Epoch 7 测试结果 ---\n",
      "  测试集 Loss: 0.8811, 测试集准确率 (Acc): 69.71%\n",
      "--------------------------\n",
      "\n",
      "  [Epoch 8/10, Batch 100/782] Loss: 0.6585 | Acc: 76.70%\n",
      "  [Epoch 8/10, Batch 200/782] Loss: 0.6627 | Acc: 76.88%\n",
      "  [Epoch 8/10, Batch 300/782] Loss: 0.6550 | Acc: 77.17%\n",
      "  [Epoch 8/10, Batch 400/782] Loss: 0.6664 | Acc: 76.83%\n",
      "  [Epoch 8/10, Batch 500/782] Loss: 0.6658 | Acc: 76.85%\n",
      "  [Epoch 8/10, Batch 600/782] Loss: 0.6652 | Acc: 76.91%\n",
      "  [Epoch 8/10, Batch 700/782] Loss: 0.6651 | Acc: 76.95%\n",
      "Epoch 8 训练完成。用时: 14.73秒\n",
      "  训练集平均 Loss: 0.6661, 训练集准确率: 76.93%\n",
      "--- Epoch 8 测试结果 ---\n",
      "  测试集 Loss: 0.8495, 测试集准确率 (Acc): 70.69%\n",
      "--------------------------\n",
      "\n",
      "  [Epoch 9/10, Batch 100/782] Loss: 0.6132 | Acc: 77.98%\n",
      "  [Epoch 9/10, Batch 200/782] Loss: 0.6198 | Acc: 77.77%\n",
      "  [Epoch 9/10, Batch 300/782] Loss: 0.6174 | Acc: 78.14%\n",
      "  [Epoch 9/10, Batch 400/782] Loss: 0.6213 | Acc: 78.25%\n",
      "  [Epoch 9/10, Batch 500/782] Loss: 0.6277 | Acc: 78.15%\n",
      "  [Epoch 9/10, Batch 600/782] Loss: 0.6244 | Acc: 78.36%\n",
      "  [Epoch 9/10, Batch 700/782] Loss: 0.6242 | Acc: 78.41%\n",
      "Epoch 9 训练完成。用时: 14.63秒\n",
      "  训练集平均 Loss: 0.6282, 训练集准确率: 78.24%\n",
      "--- Epoch 9 测试结果 ---\n",
      "  测试集 Loss: 0.8512, 测试集准确率 (Acc): 70.65%\n",
      "--------------------------\n",
      "\n",
      "  [Epoch 10/10, Batch 100/782] Loss: 0.5579 | Acc: 80.94%\n",
      "  [Epoch 10/10, Batch 200/782] Loss: 0.5674 | Acc: 80.41%\n",
      "  [Epoch 10/10, Batch 300/782] Loss: 0.5727 | Acc: 80.36%\n",
      "  [Epoch 10/10, Batch 400/782] Loss: 0.5751 | Acc: 80.15%\n",
      "  [Epoch 10/10, Batch 500/782] Loss: 0.5771 | Acc: 80.01%\n",
      "  [Epoch 10/10, Batch 600/782] Loss: 0.5792 | Acc: 79.95%\n",
      "  [Epoch 10/10, Batch 700/782] Loss: 0.5825 | Acc: 79.78%\n",
      "Epoch 10 训练完成。用时: 14.01秒\n",
      "  训练集平均 Loss: 0.5854, 训练集准确率: 79.65%\n",
      "--- Epoch 10 测试结果 ---\n",
      "  测试集 Loss: 0.8367, 测试集准确率 (Acc): 70.95%\n",
      "--------------------------\n",
      "\n",
      "=== 训练完成 ===\n",
      "在 10 轮训练后，最佳测试集准确率为: 70.95%\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------\n",
    "# 6. 开始训练\n",
    "# ----------------------------------------\n",
    "print(\"=== 开始训练 ===\")\n",
    "\n",
    "best_acc = 0.0\n",
    "for epoch in range(EPOCHS):\n",
    "    train_epoch(epoch)\n",
    "    test_acc = test_epoch(epoch)\n",
    "    \n",
    "    if test_acc > best_acc:\n",
    "        best_acc = test_acc\n",
    "\n",
    "print(f\"=== 训练完成 ===\")\n",
    "print(f\"在 {EPOCHS} 轮训练后，最佳测试集准确率为: {best_acc:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
