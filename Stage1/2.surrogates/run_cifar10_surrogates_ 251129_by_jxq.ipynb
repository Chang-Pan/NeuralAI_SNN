{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d4b160a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spikingjelly in /usr/local/lib/python3.12/dist-packages (0.0.0.0.14)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from spikingjelly) (2.9.0+cu126)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from spikingjelly) (3.10.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from spikingjelly) (2.0.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from spikingjelly) (4.67.1)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from spikingjelly) (0.24.0+cu126)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from spikingjelly) (1.16.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->spikingjelly) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->spikingjelly) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->spikingjelly) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->spikingjelly) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->spikingjelly) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->spikingjelly) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->spikingjelly) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->spikingjelly) (2.9.0.post0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->spikingjelly) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->spikingjelly) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->spikingjelly) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->spikingjelly) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch->spikingjelly) (3.6)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->spikingjelly) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch->spikingjelly) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->spikingjelly) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->spikingjelly) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->spikingjelly) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->spikingjelly) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->spikingjelly) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->spikingjelly) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->spikingjelly) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->spikingjelly) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->spikingjelly) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->spikingjelly) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch->spikingjelly) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch->spikingjelly) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->spikingjelly) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->spikingjelly) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->spikingjelly) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch->spikingjelly) (3.5.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->spikingjelly) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->spikingjelly) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->spikingjelly) (3.0.3)\n",
      "Collecting optuna\n",
      "  Downloading optuna-4.6.0-py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (1.17.2)\n",
      "Collecting colorlog (from optuna)\n",
      "  Downloading colorlog-6.10.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (25.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.44)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from optuna) (4.67.1)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from optuna) (6.0.3)\n",
      "Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (1.3.10)\n",
      "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (4.15.0)\n",
      "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.12/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.3)\n",
      "Downloading optuna-4.6.0-py3-none-any.whl (404 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m404.7/404.7 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading colorlog-6.10.1-py3-none-any.whl (11 kB)\n",
      "Collecting optuna\n",
      "  Downloading optuna-4.6.0-py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (1.17.2)\n",
      "Collecting colorlog (from optuna)\n",
      "  Downloading colorlog-6.10.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (25.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.44)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from optuna) (4.67.1)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from optuna) (6.0.3)\n",
      "Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (1.3.10)\n",
      "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (4.15.0)\n",
      "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.12/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.3)\n",
      "Downloading optuna-4.6.0-py3-none-any.whl (404 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m404.7/404.7 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading colorlog-6.10.1-py3-none-any.whl (11 kB)\n",
      "Installing collected packages: colorlog, optuna\n",
      "Installing collected packages: colorlog, optuna\n",
      "Successfully installed colorlog-6.10.1 optuna-4.6.0\n",
      "Successfully installed colorlog-6.10.1 optuna-4.6.0\n"
     ]
    }
   ],
   "source": [
    "# for colab use\n",
    "%pip install spikingjelly\n",
    "%pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1b9769a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38838a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.0+cu126\n",
      "CUDA available: True\n",
      "Current device: Tesla T4\n",
      "Device count: 1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import time\n",
    "import optuna\n",
    "from spikingjelly.activation_based import neuron, functional, layer, surrogate\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Current device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Device count: {torch.cuda.device_count()}\") # 应该输出 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "759760e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 辅助函数：阶跃函数 ---\n",
    "@torch.jit.script\n",
    "def heaviside(x: torch.Tensor):\n",
    "    \"\"\"\n",
    "    前向传播用的阶跃函数：x >= 0 时输出 1，否则输出 0\n",
    "    \"\"\"\n",
    "    return (x >= 0).float()\n",
    "\n",
    "# =========================================================\n",
    "# 1. SuperSpike 实现\n",
    "# 公式: h(x) = 1 / (beta * |x| + 1)^2\n",
    "# =========================================================\n",
    "class SuperSpikeFunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, alpha):\n",
    "        if x.requires_grad:\n",
    "            ctx.save_for_backward(x)\n",
    "            ctx.alpha = alpha\n",
    "        return heaviside(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        x, = ctx.saved_tensors\n",
    "        alpha = ctx.alpha\n",
    "        \n",
    "        # 实现图片中的公式: 1 / (beta * |x| + 1)^2\n",
    "        denom = (alpha * x.abs() + 1.0)\n",
    "        grad_x = grad_output * (1.0 / (denom * denom))\n",
    "        \n",
    "        return grad_x, None\n",
    "\n",
    "class SuperSpike(nn.Module):\n",
    "    def __init__(self, alpha=100.0, spiking=True):\n",
    "        \"\"\"\n",
    "        SuperSpike 替代梯度\n",
    "        :param alpha: 对应公式中的 beta，控制梯度的陡峭程度\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.spiking = spiking\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.spiking:\n",
    "            return SuperSpikeFunction.apply(x, self.alpha)\n",
    "        else:\n",
    "            return heaviside(x)\n",
    "\n",
    "# =========================================================\n",
    "# 2. Sigmoid' (Image Version) 实现\n",
    "# 公式: h(x) = s(x)(1 - s(x)), 其中 s(x) = sigmoid(beta * x)\n",
    "# =========================================================\n",
    "class SigmoidDerivativeFunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, alpha):\n",
    "        if x.requires_grad:\n",
    "            ctx.save_for_backward(x)\n",
    "            ctx.alpha = alpha\n",
    "        return heaviside(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        x, = ctx.saved_tensors\n",
    "        alpha = ctx.alpha\n",
    "        \n",
    "        # 计算 s(x) = sigmoid(beta * x)\n",
    "        sigmoid_x = torch.sigmoid(alpha * x)\n",
    "        \n",
    "        # 实现: h(x) = s(x) * (1 - s(x))\n",
    "        grad_x = grad_output * sigmoid_x * (1.0 - sigmoid_x)\n",
    "        \n",
    "        return grad_x, None\n",
    "\n",
    "class SigmoidDerivative(nn.Module):\n",
    "    def __init__(self, alpha=4.0, spiking=True):\n",
    "        \"\"\"\n",
    "        Sigmoid' 替代梯度 (图片版本)\n",
    "        :param alpha: 对应公式中的 beta\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.spiking = spiking\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.spiking:\n",
    "            return SigmoidDerivativeFunction.apply(x, self.alpha)\n",
    "        return heaviside(x)\n",
    "\n",
    "# =========================================================\n",
    "# 3. Esser et al. 实现\n",
    "# 公式: h(x) = max(0, 1.0 - beta * |x|)\n",
    "# =========================================================\n",
    "class EsserFunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, alpha):\n",
    "        if x.requires_grad:\n",
    "            ctx.save_for_backward(x)\n",
    "            ctx.alpha = alpha\n",
    "        return heaviside(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        x, = ctx.saved_tensors\n",
    "        alpha = ctx.alpha\n",
    "        \n",
    "        # 实现图片公式: max(0, 1.0 - beta * |x|)\n",
    "        grad_x = grad_output * torch.clamp(1.0 - alpha * x.abs(), min=0.0)\n",
    "        \n",
    "        return grad_x, None\n",
    "\n",
    "class Esser(nn.Module):\n",
    "    def __init__(self, alpha=1.0, spiking=True):\n",
    "        \"\"\"\n",
    "        Esser et al. 替代梯度\n",
    "        :param alpha: 对应公式中的 beta，通常设为 1.0 或更大\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.spiking = spiking\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.spiking:\n",
    "            return EsserFunction.apply(x, self.alpha)\n",
    "        return heaviside(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "002a6234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 实验设置 ---\n",
      "设备 (DEVICE): cuda:0\n",
      "仿真时长 (T): 8\n",
      "批大小 (BATCH_SIZE): 64\n",
      "训练轮数 (EPOCHS): 10\n",
      "------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------\n",
    "# 1. 定义超参数和设置\n",
    "# ----------------------------------------\n",
    "\n",
    "T = 8             # 仿真总时长 (SNN 的关键参数)\n",
    "BATCH_SIZE = 64   # 批处理大小\n",
    "EPOCHS = 10       # 训练轮数 (为快速演示，设置较小)\n",
    "LR = 1e-3         # 学习率\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "BETA = 10.0       # 替代梯度中的超参数, 论文中规定值\n",
    "\n",
    "print(f\"--- 实验设置 ---\")\n",
    "print(f\"设备 (DEVICE): {DEVICE}\")\n",
    "print(f\"仿真时长 (T): {T}\")\n",
    "print(f\"批大小 (BATCH_SIZE): {BATCH_SIZE}\")\n",
    "print(f\"训练轮数 (EPOCHS): {EPOCHS}\")\n",
    "print(f\"------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b391e614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在加载 CIFAR10 数据集...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170M/170M [00:05<00:00, 32.1MB/s] \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据集加载完毕。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------\n",
    "# 2. 加载和预处理 CIFAR10 数据集\n",
    "# ----------------------------------------\n",
    "print(\"正在加载 CIFAR10 数据集...\")\n",
    "# CIFAR10 图像的均值和标准差 (用于归一化)\n",
    "cifar_mean = (0.4914, 0.4822, 0.4465)\n",
    "cifar_std = (0.2023, 0.1994, 0.2010)\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(), # 简单数据增强：随机翻转\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(cifar_mean, cifar_std)\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(cifar_mean, cifar_std)\n",
    "])\n",
    "\n",
    "# 加载数据\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "print(\"数据集加载完毕。\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86696b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------\n",
    "# 3. 定义基础的卷积 SNN 模型\n",
    "# ----------------------------------------\n",
    "# 使用 nn.Sequential 快速搭建一个简单的 CNN 结构\n",
    "# 关键在于在激活函数的位置换上 SNN 的脉冲神经元\n",
    "\n",
    "class BasicCSNN(nn.Module):\n",
    "    # 增加 surrogate_function 参数\n",
    "    def __init__(self, T, surrogate_function=surrogate.Sigmoid()):\n",
    "        super().__init__()\n",
    "        self.T = T  # 保存仿真时长\n",
    "        print(f\"Initializing Network with Surrogate: {surrogate_function.__class__.__name__}\")\n",
    "\n",
    "        # 定义网络结构\n",
    "        # 结构：[卷积 -> 脉冲 -> 池化] x 2 -> [展平 -> 全连接 -> 脉冲] -> [全连接]\n",
    "        self.net = nn.Sequential(\n",
    "            # 块 1\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            # --- 核心：使用 LIF 神经元 ---\n",
    "            # 激活驱动:LIFNode 在前向传播时模拟 LIF 神经元动力学，在反向传播时，SpikingJelly 会自动使用“替代梯度”进行计算。\n",
    "            neuron.LIFNode(surrogate_function=surrogate_function),\n",
    "            nn.MaxPool2d(2),  # 32x32 -> 16x16\n",
    "\n",
    "            # 块 2\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            neuron.LIFNode(surrogate_function=surrogate_function),\n",
    "            nn.MaxPool2d(2),  # 16x16 -> 8x8\n",
    "\n",
    "            # 展平\n",
    "            nn.Flatten(),\n",
    "\n",
    "            # 全连接层 1\n",
    "            nn.Linear(64 * 8 * 8, 128), # 64 * 8 * 8 = 4096\n",
    "            neuron.LIFNode(surrogate_function=surrogate_function),\n",
    "\n",
    "            # 输出层 (全连接层 2)\n",
    "            # 输出层通常不使用脉冲神经元，而是直接输出膜电位或累积电流\n",
    "            # 这样可以方便地与交叉熵损失配合使用\n",
    "            nn.Linear(128, 10) # 10个类别\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # --- SNN 算法思路的核心 ---\n",
    "        # SNN 神经元是有状态的（例如膜电位 V），在处理一个新样本前必须重置\n",
    "        # 1. 重置网络中所有神经元的状态\n",
    "        functional.reset_net(self)\n",
    "\n",
    "        # 准备一个列表来收集 T 个时间步的输出\n",
    "        # (T, N, C)，T=时间步, N=BatchSize, C=类别数\n",
    "        outputs_over_time = []\n",
    "\n",
    "        # 2. SNN 的时间步循环\n",
    "        # 对于静态图像 (如CIFAR10)，我们在 T 个时间步内输入 *相同* 的图像 x\n",
    "        # 神经元会在这 T 步内不断累积输入并发放脉冲\n",
    "        for t in range(self.T):\n",
    "            # 运行一步前向传播\n",
    "            out_t = self.net(x)\n",
    "            outputs_over_time.append(out_t)\n",
    "\n",
    "        # 3. 聚合 T 个时间步的输出\n",
    "        # (T, N, 10) -> (T, N, 10)\n",
    "        outputs_stack = torch.stack(outputs_over_time)\n",
    "        \n",
    "        # 4. 解码：计算 T 步内的平均输出\n",
    "        # (T, N, 10) -> (N, 10)\n",
    "        # 我们取所有时间步输出的平均值，作为最终的分类 \"logits\"\n",
    "        # 这是一种常见的 SNN 解码方式（Rate Coding / Mean Output）\n",
    "        return outputs_stack.mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf29bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "实验配置已更新为 Optuna 搜索模式。\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------\n",
    "# 4. 准备实验配置 (Optuna 版本)\n",
    "# ----------------------------------------\n",
    "\n",
    "# 定义要对比的替代梯度名称\n",
    "surrogate_types = [\"SuperSpike\", \"Sigmoid\", \"Esser\"]\n",
    "\n",
    "# 辅助函数：根据名称和 alpha 创建 surrogate 实例\n",
    "def get_surrogate_func(name, alpha):\n",
    "    if name == \"SuperSpike\":\n",
    "        return SuperSpike(alpha=alpha)\n",
    "    elif name == \"Sigmoid\":\n",
    "        return SigmoidDerivative(alpha=alpha)\n",
    "    elif name == \"Esser\":\n",
    "        return Esser(alpha=alpha)\n",
    "    raise ValueError(f\"Unknown surrogate type: {name}\")\n",
    "\n",
    "# 定义损失函数 (全局使用)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d5d6e739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------\n",
    "# 5. 训练和评估循环\n",
    "# ----------------------------------------\n",
    "\n",
    "# --- 训练函数 (Train Loop) ---\n",
    "def train_epoch(model, optimizer, epoch, model_name):\n",
    "    model.train()  # 设置为训练模式\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs) # 这里的 model 是参数传进来的\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        # 统计损失和准确率\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "\n",
    "    end_time = time.time()\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    acc = 100. * correct / total\n",
    "    print(f\"[{model_name}] Epoch {epoch+1} Train | Loss: {avg_loss:.4f} | Acc: {acc:.2f}% | Time: {end_time - start_time:.2f}s\")\n",
    "    return avg_loss, acc\n",
    "\n",
    "# --- 评估函数 (Eval Loop) ---\n",
    "def test_epoch(model, epoch, model_name):\n",
    "    model.eval()  # 设置为评估模式\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # 评估时不需要计算梯度\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "\n",
    "            # 前向传播\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # 计算损失\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # 统计准确率\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            \n",
    "    acc = 100. * correct / total\n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    print(f\"[{model_name}] Epoch {epoch+1} Test  | Loss: {avg_loss:.4f} | Acc: {acc:.2f}%\")\n",
    "    return avg_loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f8600fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-08 13:45:40,599] A new study created in memory with name: no-name-e9a68d55-b097-41c1-92d0-06e5cefa2a4b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 开始对比实验 (总 Epochs: 10) ===\n",
      "\n",
      "\n",
      ">>> 开始搜索 SuperSpike 的最优 alpha (Trials: 10) ...\n",
      "Initializing Network with Surrogate: SuperSpike\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SuperSpike_alpha=6.52] Epoch 1 Train | Loss: 1.5638 | Acc: 43.48% | Time: 34.65s\n",
      "[SuperSpike_alpha=6.52] Epoch 1 Test  | Loss: 1.2688 | Acc: 55.07%\n",
      "[SuperSpike_alpha=6.52] Epoch 1 Test  | Loss: 1.2688 | Acc: 55.07%\n",
      "[SuperSpike_alpha=6.52] Epoch 2 Train | Loss: 1.1794 | Acc: 58.07% | Time: 33.41s\n",
      "[SuperSpike_alpha=6.52] Epoch 2 Train | Loss: 1.1794 | Acc: 58.07% | Time: 33.41s\n",
      "[SuperSpike_alpha=6.52] Epoch 2 Test  | Loss: 1.1129 | Acc: 60.03%\n",
      "[SuperSpike_alpha=6.52] Epoch 2 Test  | Loss: 1.1129 | Acc: 60.03%\n",
      "[SuperSpike_alpha=6.52] Epoch 3 Train | Loss: 1.0209 | Acc: 63.89% | Time: 33.31s\n",
      "[SuperSpike_alpha=6.52] Epoch 3 Train | Loss: 1.0209 | Acc: 63.89% | Time: 33.31s\n",
      "[SuperSpike_alpha=6.52] Epoch 3 Test  | Loss: 0.9813 | Acc: 64.96%\n",
      "[SuperSpike_alpha=6.52] Epoch 3 Test  | Loss: 0.9813 | Acc: 64.96%\n",
      "[SuperSpike_alpha=6.52] Epoch 4 Train | Loss: 0.9114 | Acc: 68.15% | Time: 33.11s\n",
      "[SuperSpike_alpha=6.52] Epoch 4 Train | Loss: 0.9114 | Acc: 68.15% | Time: 33.11s\n",
      "[SuperSpike_alpha=6.52] Epoch 4 Test  | Loss: 0.9235 | Acc: 67.64%\n",
      "[SuperSpike_alpha=6.52] Epoch 4 Test  | Loss: 0.9235 | Acc: 67.64%\n",
      "[SuperSpike_alpha=6.52] Epoch 5 Train | Loss: 0.8395 | Acc: 70.86% | Time: 32.79s\n",
      "[SuperSpike_alpha=6.52] Epoch 5 Train | Loss: 0.8395 | Acc: 70.86% | Time: 32.79s\n",
      "[SuperSpike_alpha=6.52] Epoch 5 Test  | Loss: 0.8960 | Acc: 68.60%\n",
      "[SuperSpike_alpha=6.52] Epoch 5 Test  | Loss: 0.8960 | Acc: 68.60%\n",
      "[SuperSpike_alpha=6.52] Epoch 6 Train | Loss: 0.7734 | Acc: 73.16% | Time: 32.65s\n",
      "[SuperSpike_alpha=6.52] Epoch 6 Train | Loss: 0.7734 | Acc: 73.16% | Time: 32.65s\n",
      "[SuperSpike_alpha=6.52] Epoch 6 Test  | Loss: 0.8709 | Acc: 69.41%\n",
      "[SuperSpike_alpha=6.52] Epoch 6 Test  | Loss: 0.8709 | Acc: 69.41%\n",
      "[SuperSpike_alpha=6.52] Epoch 7 Train | Loss: 0.7265 | Acc: 74.66% | Time: 33.25s\n",
      "[SuperSpike_alpha=6.52] Epoch 7 Train | Loss: 0.7265 | Acc: 74.66% | Time: 33.25s\n",
      "[SuperSpike_alpha=6.52] Epoch 7 Test  | Loss: 0.8510 | Acc: 70.38%\n",
      "[SuperSpike_alpha=6.52] Epoch 7 Test  | Loss: 0.8510 | Acc: 70.38%\n",
      "[SuperSpike_alpha=6.52] Epoch 8 Train | Loss: 0.6796 | Acc: 76.54% | Time: 33.40s\n",
      "[SuperSpike_alpha=6.52] Epoch 8 Train | Loss: 0.6796 | Acc: 76.54% | Time: 33.40s\n",
      "[SuperSpike_alpha=6.52] Epoch 8 Test  | Loss: 0.8257 | Acc: 71.78%\n",
      "[SuperSpike_alpha=6.52] Epoch 8 Test  | Loss: 0.8257 | Acc: 71.78%\n",
      "[SuperSpike_alpha=6.52] Epoch 9 Train | Loss: 0.6367 | Acc: 77.91% | Time: 33.14s\n",
      "[SuperSpike_alpha=6.52] Epoch 9 Train | Loss: 0.6367 | Acc: 77.91% | Time: 33.14s\n",
      "[SuperSpike_alpha=6.52] Epoch 9 Test  | Loss: 0.8145 | Acc: 72.29%\n",
      "[SuperSpike_alpha=6.52] Epoch 9 Test  | Loss: 0.8145 | Acc: 72.29%\n",
      "[SuperSpike_alpha=6.52] Epoch 10 Train | Loss: 0.5959 | Acc: 79.42% | Time: 33.41s\n",
      "[SuperSpike_alpha=6.52] Epoch 10 Train | Loss: 0.5959 | Acc: 79.42% | Time: 33.41s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-08 13:51:51,334] Trial 0 finished with value: 72.29 and parameters: {'alpha': 6.523310167940246}. Best is trial 0 with value: 72.29.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SuperSpike_alpha=6.52] Epoch 10 Test  | Loss: 0.8286 | Acc: 71.50%\n",
      "Initializing Network with Surrogate: SuperSpike\n",
      "[SuperSpike_alpha=15.21] Epoch 1 Train | Loss: 1.6025 | Acc: 42.12% | Time: 33.65s\n",
      "[SuperSpike_alpha=15.21] Epoch 1 Train | Loss: 1.6025 | Acc: 42.12% | Time: 33.65s\n",
      "[SuperSpike_alpha=15.21] Epoch 1 Test  | Loss: 1.3072 | Acc: 53.06%\n",
      "[SuperSpike_alpha=15.21] Epoch 1 Test  | Loss: 1.3072 | Acc: 53.06%\n",
      "[SuperSpike_alpha=15.21] Epoch 2 Train | Loss: 1.2141 | Acc: 56.57% | Time: 33.61s\n",
      "[SuperSpike_alpha=15.21] Epoch 2 Train | Loss: 1.2141 | Acc: 56.57% | Time: 33.61s\n",
      "[SuperSpike_alpha=15.21] Epoch 2 Test  | Loss: 1.1002 | Acc: 60.52%\n",
      "[SuperSpike_alpha=15.21] Epoch 2 Test  | Loss: 1.1002 | Acc: 60.52%\n",
      "[SuperSpike_alpha=15.21] Epoch 3 Train | Loss: 1.0424 | Acc: 63.11% | Time: 33.77s\n",
      "[SuperSpike_alpha=15.21] Epoch 3 Train | Loss: 1.0424 | Acc: 63.11% | Time: 33.77s\n",
      "[SuperSpike_alpha=15.21] Epoch 3 Test  | Loss: 1.0061 | Acc: 64.30%\n",
      "[SuperSpike_alpha=15.21] Epoch 3 Test  | Loss: 1.0061 | Acc: 64.30%\n",
      "[SuperSpike_alpha=15.21] Epoch 4 Train | Loss: 0.9365 | Acc: 67.03% | Time: 33.91s\n",
      "[SuperSpike_alpha=15.21] Epoch 4 Train | Loss: 0.9365 | Acc: 67.03% | Time: 33.91s\n",
      "[SuperSpike_alpha=15.21] Epoch 4 Test  | Loss: 0.9388 | Acc: 66.71%\n",
      "[SuperSpike_alpha=15.21] Epoch 4 Test  | Loss: 0.9388 | Acc: 66.71%\n",
      "[SuperSpike_alpha=15.21] Epoch 5 Train | Loss: 0.8662 | Acc: 69.58% | Time: 34.19s\n",
      "[SuperSpike_alpha=15.21] Epoch 5 Train | Loss: 0.8662 | Acc: 69.58% | Time: 34.19s\n",
      "[SuperSpike_alpha=15.21] Epoch 5 Test  | Loss: 0.8945 | Acc: 68.40%\n",
      "[SuperSpike_alpha=15.21] Epoch 5 Test  | Loss: 0.8945 | Acc: 68.40%\n",
      "[SuperSpike_alpha=15.21] Epoch 6 Train | Loss: 0.8065 | Acc: 71.71% | Time: 33.94s\n",
      "[SuperSpike_alpha=15.21] Epoch 6 Train | Loss: 0.8065 | Acc: 71.71% | Time: 33.94s\n",
      "[SuperSpike_alpha=15.21] Epoch 6 Test  | Loss: 0.8621 | Acc: 69.66%\n",
      "[SuperSpike_alpha=15.21] Epoch 6 Test  | Loss: 0.8621 | Acc: 69.66%\n",
      "[SuperSpike_alpha=15.21] Epoch 7 Train | Loss: 0.7582 | Acc: 73.63% | Time: 34.04s\n",
      "[SuperSpike_alpha=15.21] Epoch 7 Train | Loss: 0.7582 | Acc: 73.63% | Time: 34.04s\n",
      "[SuperSpike_alpha=15.21] Epoch 7 Test  | Loss: 0.8402 | Acc: 70.22%\n",
      "[SuperSpike_alpha=15.21] Epoch 7 Test  | Loss: 0.8402 | Acc: 70.22%\n",
      "[SuperSpike_alpha=15.21] Epoch 8 Train | Loss: 0.7195 | Acc: 74.83% | Time: 34.01s\n",
      "[SuperSpike_alpha=15.21] Epoch 8 Train | Loss: 0.7195 | Acc: 74.83% | Time: 34.01s\n",
      "[SuperSpike_alpha=15.21] Epoch 8 Test  | Loss: 0.8331 | Acc: 70.98%\n",
      "[SuperSpike_alpha=15.21] Epoch 8 Test  | Loss: 0.8331 | Acc: 70.98%\n",
      "[SuperSpike_alpha=15.21] Epoch 9 Train | Loss: 0.6849 | Acc: 76.24% | Time: 34.05s\n",
      "[SuperSpike_alpha=15.21] Epoch 9 Train | Loss: 0.6849 | Acc: 76.24% | Time: 34.05s\n",
      "[SuperSpike_alpha=15.21] Epoch 9 Test  | Loss: 0.8150 | Acc: 71.78%\n",
      "[SuperSpike_alpha=15.21] Epoch 9 Test  | Loss: 0.8150 | Acc: 71.78%\n",
      "[SuperSpike_alpha=15.21] Epoch 10 Train | Loss: 0.6442 | Acc: 77.43% | Time: 34.11s\n",
      "[SuperSpike_alpha=15.21] Epoch 10 Train | Loss: 0.6442 | Acc: 77.43% | Time: 34.11s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-08 13:58:04,249] Trial 1 finished with value: 72.1 and parameters: {'alpha': 15.209298042081468}. Best is trial 0 with value: 72.29.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SuperSpike_alpha=15.21] Epoch 10 Test  | Loss: 0.8162 | Acc: 72.10%\n",
      "Initializing Network with Surrogate: SuperSpike\n",
      "[SuperSpike_alpha=4.67] Epoch 1 Train | Loss: 1.5721 | Acc: 43.02% | Time: 34.04s\n",
      "[SuperSpike_alpha=4.67] Epoch 1 Train | Loss: 1.5721 | Acc: 43.02% | Time: 34.04s\n",
      "[SuperSpike_alpha=4.67] Epoch 1 Test  | Loss: 1.3004 | Acc: 53.19%\n",
      "[SuperSpike_alpha=4.67] Epoch 1 Test  | Loss: 1.3004 | Acc: 53.19%\n",
      "[SuperSpike_alpha=4.67] Epoch 2 Train | Loss: 1.1885 | Acc: 57.76% | Time: 34.02s\n",
      "[SuperSpike_alpha=4.67] Epoch 2 Train | Loss: 1.1885 | Acc: 57.76% | Time: 34.02s\n",
      "[SuperSpike_alpha=4.67] Epoch 2 Test  | Loss: 1.0914 | Acc: 61.41%\n",
      "[SuperSpike_alpha=4.67] Epoch 2 Test  | Loss: 1.0914 | Acc: 61.41%\n",
      "[SuperSpike_alpha=4.67] Epoch 3 Train | Loss: 1.0196 | Acc: 63.92% | Time: 34.11s\n",
      "[SuperSpike_alpha=4.67] Epoch 3 Train | Loss: 1.0196 | Acc: 63.92% | Time: 34.11s\n",
      "[SuperSpike_alpha=4.67] Epoch 3 Test  | Loss: 0.9917 | Acc: 65.03%\n",
      "[SuperSpike_alpha=4.67] Epoch 3 Test  | Loss: 0.9917 | Acc: 65.03%\n",
      "[SuperSpike_alpha=4.67] Epoch 4 Train | Loss: 0.9003 | Acc: 68.48% | Time: 33.94s\n",
      "[SuperSpike_alpha=4.67] Epoch 4 Train | Loss: 0.9003 | Acc: 68.48% | Time: 33.94s\n",
      "[SuperSpike_alpha=4.67] Epoch 4 Test  | Loss: 0.9240 | Acc: 67.65%\n",
      "[SuperSpike_alpha=4.67] Epoch 4 Test  | Loss: 0.9240 | Acc: 67.65%\n",
      "[SuperSpike_alpha=4.67] Epoch 5 Train | Loss: 0.8229 | Acc: 71.32% | Time: 34.01s\n",
      "[SuperSpike_alpha=4.67] Epoch 5 Train | Loss: 0.8229 | Acc: 71.32% | Time: 34.01s\n",
      "[SuperSpike_alpha=4.67] Epoch 5 Test  | Loss: 0.9297 | Acc: 67.28%\n",
      "[SuperSpike_alpha=4.67] Epoch 5 Test  | Loss: 0.9297 | Acc: 67.28%\n",
      "[SuperSpike_alpha=4.67] Epoch 6 Train | Loss: 0.7604 | Acc: 73.49% | Time: 34.05s\n",
      "[SuperSpike_alpha=4.67] Epoch 6 Train | Loss: 0.7604 | Acc: 73.49% | Time: 34.05s\n",
      "[SuperSpike_alpha=4.67] Epoch 6 Test  | Loss: 0.8502 | Acc: 70.39%\n",
      "[SuperSpike_alpha=4.67] Epoch 6 Test  | Loss: 0.8502 | Acc: 70.39%\n",
      "[SuperSpike_alpha=4.67] Epoch 7 Train | Loss: 0.7064 | Acc: 75.54% | Time: 34.01s\n",
      "[SuperSpike_alpha=4.67] Epoch 7 Train | Loss: 0.7064 | Acc: 75.54% | Time: 34.01s\n",
      "[SuperSpike_alpha=4.67] Epoch 7 Test  | Loss: 0.8431 | Acc: 70.84%\n",
      "[SuperSpike_alpha=4.67] Epoch 7 Test  | Loss: 0.8431 | Acc: 70.84%\n",
      "[SuperSpike_alpha=4.67] Epoch 8 Train | Loss: 0.6596 | Acc: 76.88% | Time: 34.05s\n",
      "[SuperSpike_alpha=4.67] Epoch 8 Train | Loss: 0.6596 | Acc: 76.88% | Time: 34.05s\n",
      "[SuperSpike_alpha=4.67] Epoch 8 Test  | Loss: 0.8396 | Acc: 71.30%\n",
      "[SuperSpike_alpha=4.67] Epoch 8 Test  | Loss: 0.8396 | Acc: 71.30%\n",
      "[SuperSpike_alpha=4.67] Epoch 9 Train | Loss: 0.6182 | Acc: 78.55% | Time: 33.87s\n",
      "[SuperSpike_alpha=4.67] Epoch 9 Train | Loss: 0.6182 | Acc: 78.55% | Time: 33.87s\n",
      "[SuperSpike_alpha=4.67] Epoch 9 Test  | Loss: 0.8315 | Acc: 71.41%\n",
      "[SuperSpike_alpha=4.67] Epoch 9 Test  | Loss: 0.8315 | Acc: 71.41%\n",
      "[SuperSpike_alpha=4.67] Epoch 10 Train | Loss: 0.5750 | Acc: 80.15% | Time: 33.93s\n",
      "[SuperSpike_alpha=4.67] Epoch 10 Train | Loss: 0.5750 | Acc: 80.15% | Time: 33.93s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-08 14:04:16,540] Trial 2 finished with value: 72.28 and parameters: {'alpha': 4.665509544634646}. Best is trial 0 with value: 72.29.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SuperSpike_alpha=4.67] Epoch 10 Test  | Loss: 0.8075 | Acc: 72.28%\n",
      "Initializing Network with Surrogate: SuperSpike\n",
      "[SuperSpike_alpha=10.95] Epoch 1 Train | Loss: 1.6272 | Acc: 41.40% | Time: 33.83s\n",
      "[SuperSpike_alpha=10.95] Epoch 1 Train | Loss: 1.6272 | Acc: 41.40% | Time: 33.83s\n",
      "[SuperSpike_alpha=10.95] Epoch 1 Test  | Loss: 1.3093 | Acc: 52.95%\n",
      "[SuperSpike_alpha=10.95] Epoch 1 Test  | Loss: 1.3093 | Acc: 52.95%\n",
      "[SuperSpike_alpha=10.95] Epoch 2 Train | Loss: 1.2227 | Acc: 55.97% | Time: 33.88s\n",
      "[SuperSpike_alpha=10.95] Epoch 2 Train | Loss: 1.2227 | Acc: 55.97% | Time: 33.88s\n",
      "[SuperSpike_alpha=10.95] Epoch 2 Test  | Loss: 1.1452 | Acc: 59.10%\n",
      "[SuperSpike_alpha=10.95] Epoch 2 Test  | Loss: 1.1452 | Acc: 59.10%\n",
      "[SuperSpike_alpha=10.95] Epoch 3 Train | Loss: 1.0758 | Acc: 61.74% | Time: 33.86s\n",
      "[SuperSpike_alpha=10.95] Epoch 3 Train | Loss: 1.0758 | Acc: 61.74% | Time: 33.86s\n",
      "[SuperSpike_alpha=10.95] Epoch 3 Test  | Loss: 1.0424 | Acc: 63.42%\n",
      "[SuperSpike_alpha=10.95] Epoch 3 Test  | Loss: 1.0424 | Acc: 63.42%\n",
      "[SuperSpike_alpha=10.95] Epoch 4 Train | Loss: 0.9685 | Acc: 65.66% | Time: 33.96s\n",
      "[SuperSpike_alpha=10.95] Epoch 4 Train | Loss: 0.9685 | Acc: 65.66% | Time: 33.96s\n",
      "[SuperSpike_alpha=10.95] Epoch 4 Test  | Loss: 0.9672 | Acc: 65.70%\n",
      "[SuperSpike_alpha=10.95] Epoch 4 Test  | Loss: 0.9672 | Acc: 65.70%\n",
      "[SuperSpike_alpha=10.95] Epoch 5 Train | Loss: 0.8907 | Acc: 68.67% | Time: 33.82s\n",
      "[SuperSpike_alpha=10.95] Epoch 5 Train | Loss: 0.8907 | Acc: 68.67% | Time: 33.82s\n",
      "[SuperSpike_alpha=10.95] Epoch 5 Test  | Loss: 0.9161 | Acc: 67.88%\n",
      "[SuperSpike_alpha=10.95] Epoch 5 Test  | Loss: 0.9161 | Acc: 67.88%\n",
      "[SuperSpike_alpha=10.95] Epoch 6 Train | Loss: 0.8312 | Acc: 70.82% | Time: 34.03s\n",
      "[SuperSpike_alpha=10.95] Epoch 6 Train | Loss: 0.8312 | Acc: 70.82% | Time: 34.03s\n",
      "[SuperSpike_alpha=10.95] Epoch 6 Test  | Loss: 0.8942 | Acc: 68.34%\n",
      "[SuperSpike_alpha=10.95] Epoch 6 Test  | Loss: 0.8942 | Acc: 68.34%\n",
      "[SuperSpike_alpha=10.95] Epoch 7 Train | Loss: 0.7821 | Acc: 72.72% | Time: 33.95s\n",
      "[SuperSpike_alpha=10.95] Epoch 7 Train | Loss: 0.7821 | Acc: 72.72% | Time: 33.95s\n",
      "[SuperSpike_alpha=10.95] Epoch 7 Test  | Loss: 0.8750 | Acc: 69.33%\n",
      "[SuperSpike_alpha=10.95] Epoch 7 Test  | Loss: 0.8750 | Acc: 69.33%\n",
      "[SuperSpike_alpha=10.95] Epoch 8 Train | Loss: 0.7370 | Acc: 74.38% | Time: 33.96s\n",
      "[SuperSpike_alpha=10.95] Epoch 8 Train | Loss: 0.7370 | Acc: 74.38% | Time: 33.96s\n",
      "[SuperSpike_alpha=10.95] Epoch 8 Test  | Loss: 0.8477 | Acc: 70.25%\n",
      "[SuperSpike_alpha=10.95] Epoch 8 Test  | Loss: 0.8477 | Acc: 70.25%\n",
      "[SuperSpike_alpha=10.95] Epoch 9 Train | Loss: 0.6970 | Acc: 75.69% | Time: 33.95s\n",
      "[SuperSpike_alpha=10.95] Epoch 9 Train | Loss: 0.6970 | Acc: 75.69% | Time: 33.95s\n",
      "[SuperSpike_alpha=10.95] Epoch 9 Test  | Loss: 0.8401 | Acc: 70.27%\n",
      "[SuperSpike_alpha=10.95] Epoch 9 Test  | Loss: 0.8401 | Acc: 70.27%\n",
      "[SuperSpike_alpha=10.95] Epoch 10 Train | Loss: 0.6638 | Acc: 76.95% | Time: 34.11s\n",
      "[SuperSpike_alpha=10.95] Epoch 10 Train | Loss: 0.6638 | Acc: 76.95% | Time: 34.11s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-08 14:10:28,133] Trial 3 finished with value: 70.85 and parameters: {'alpha': 10.954255321610162}. Best is trial 0 with value: 72.29.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SuperSpike_alpha=10.95] Epoch 10 Test  | Loss: 0.8303 | Acc: 70.85%\n",
      "Initializing Network with Surrogate: SuperSpike\n",
      "[SuperSpike_alpha=16.52] Epoch 1 Train | Loss: 1.5931 | Acc: 42.57% | Time: 33.94s\n",
      "[SuperSpike_alpha=16.52] Epoch 1 Train | Loss: 1.5931 | Acc: 42.57% | Time: 33.94s\n",
      "[SuperSpike_alpha=16.52] Epoch 1 Test  | Loss: 1.2678 | Acc: 54.66%\n",
      "[SuperSpike_alpha=16.52] Epoch 1 Test  | Loss: 1.2678 | Acc: 54.66%\n",
      "[SuperSpike_alpha=16.52] Epoch 2 Train | Loss: 1.1743 | Acc: 58.41% | Time: 34.03s\n",
      "[SuperSpike_alpha=16.52] Epoch 2 Train | Loss: 1.1743 | Acc: 58.41% | Time: 34.03s\n",
      "[SuperSpike_alpha=16.52] Epoch 2 Test  | Loss: 1.0749 | Acc: 61.52%\n",
      "[SuperSpike_alpha=16.52] Epoch 2 Test  | Loss: 1.0749 | Acc: 61.52%\n",
      "[SuperSpike_alpha=16.52] Epoch 3 Train | Loss: 1.0227 | Acc: 63.90% | Time: 34.09s\n",
      "[SuperSpike_alpha=16.52] Epoch 3 Train | Loss: 1.0227 | Acc: 63.90% | Time: 34.09s\n",
      "[SuperSpike_alpha=16.52] Epoch 3 Test  | Loss: 0.9950 | Acc: 65.21%\n",
      "[SuperSpike_alpha=16.52] Epoch 3 Test  | Loss: 0.9950 | Acc: 65.21%\n",
      "[SuperSpike_alpha=16.52] Epoch 4 Train | Loss: 0.9272 | Acc: 67.60% | Time: 34.35s\n",
      "[SuperSpike_alpha=16.52] Epoch 4 Train | Loss: 0.9272 | Acc: 67.60% | Time: 34.35s\n",
      "[SuperSpike_alpha=16.52] Epoch 4 Test  | Loss: 0.9283 | Acc: 67.58%\n",
      "[SuperSpike_alpha=16.52] Epoch 4 Test  | Loss: 0.9283 | Acc: 67.58%\n",
      "[SuperSpike_alpha=16.52] Epoch 5 Train | Loss: 0.8600 | Acc: 70.00% | Time: 34.52s\n",
      "[SuperSpike_alpha=16.52] Epoch 5 Train | Loss: 0.8600 | Acc: 70.00% | Time: 34.52s\n",
      "[SuperSpike_alpha=16.52] Epoch 5 Test  | Loss: 0.9098 | Acc: 68.24%\n",
      "[SuperSpike_alpha=16.52] Epoch 5 Test  | Loss: 0.9098 | Acc: 68.24%\n",
      "[SuperSpike_alpha=16.52] Epoch 6 Train | Loss: 0.8068 | Acc: 71.93% | Time: 34.18s\n",
      "[SuperSpike_alpha=16.52] Epoch 6 Train | Loss: 0.8068 | Acc: 71.93% | Time: 34.18s\n",
      "[SuperSpike_alpha=16.52] Epoch 6 Test  | Loss: 0.8685 | Acc: 69.80%\n",
      "[SuperSpike_alpha=16.52] Epoch 6 Test  | Loss: 0.8685 | Acc: 69.80%\n",
      "[SuperSpike_alpha=16.52] Epoch 7 Train | Loss: 0.7668 | Acc: 73.17% | Time: 34.33s\n",
      "[SuperSpike_alpha=16.52] Epoch 7 Train | Loss: 0.7668 | Acc: 73.17% | Time: 34.33s\n",
      "[SuperSpike_alpha=16.52] Epoch 7 Test  | Loss: 0.8687 | Acc: 70.28%\n",
      "[SuperSpike_alpha=16.52] Epoch 7 Test  | Loss: 0.8687 | Acc: 70.28%\n",
      "[SuperSpike_alpha=16.52] Epoch 8 Train | Loss: 0.7284 | Acc: 74.44% | Time: 34.29s\n",
      "[SuperSpike_alpha=16.52] Epoch 8 Train | Loss: 0.7284 | Acc: 74.44% | Time: 34.29s\n",
      "[SuperSpike_alpha=16.52] Epoch 8 Test  | Loss: 0.8430 | Acc: 70.71%\n",
      "[SuperSpike_alpha=16.52] Epoch 8 Test  | Loss: 0.8430 | Acc: 70.71%\n",
      "[SuperSpike_alpha=16.52] Epoch 9 Train | Loss: 0.6923 | Acc: 75.86% | Time: 34.21s\n",
      "[SuperSpike_alpha=16.52] Epoch 9 Train | Loss: 0.6923 | Acc: 75.86% | Time: 34.21s\n",
      "[SuperSpike_alpha=16.52] Epoch 9 Test  | Loss: 0.8279 | Acc: 71.63%\n",
      "[SuperSpike_alpha=16.52] Epoch 9 Test  | Loss: 0.8279 | Acc: 71.63%\n",
      "[SuperSpike_alpha=16.52] Epoch 10 Train | Loss: 0.6643 | Acc: 76.79% | Time: 34.43s\n",
      "[SuperSpike_alpha=16.52] Epoch 10 Train | Loss: 0.6643 | Acc: 76.79% | Time: 34.43s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-08 14:16:43,116] Trial 4 finished with value: 71.63 and parameters: {'alpha': 16.517890241760902}. Best is trial 0 with value: 72.29.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SuperSpike_alpha=16.52] Epoch 10 Test  | Loss: 0.8277 | Acc: 71.23%\n",
      "Initializing Network with Surrogate: SuperSpike\n",
      "[SuperSpike_alpha=10.77] Epoch 1 Train | Loss: 1.6024 | Acc: 41.93% | Time: 34.24s\n",
      "[SuperSpike_alpha=10.77] Epoch 1 Train | Loss: 1.6024 | Acc: 41.93% | Time: 34.24s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-08 14:17:20,637] Trial 5 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SuperSpike_alpha=10.77] Epoch 1 Test  | Loss: 1.3224 | Acc: 52.72%\n",
      "Initializing Network with Surrogate: SuperSpike\n",
      "[SuperSpike_alpha=5.30] Epoch 1 Train | Loss: 1.5474 | Acc: 44.13% | Time: 34.28s\n",
      "[SuperSpike_alpha=5.30] Epoch 1 Train | Loss: 1.5474 | Acc: 44.13% | Time: 34.28s\n",
      "[SuperSpike_alpha=5.30] Epoch 1 Test  | Loss: 1.2525 | Acc: 54.37%\n",
      "[SuperSpike_alpha=5.30] Epoch 1 Test  | Loss: 1.2525 | Acc: 54.37%\n",
      "[SuperSpike_alpha=5.30] Epoch 2 Train | Loss: 1.1527 | Acc: 59.11% | Time: 34.22s\n",
      "[SuperSpike_alpha=5.30] Epoch 2 Train | Loss: 1.1527 | Acc: 59.11% | Time: 34.22s\n",
      "[SuperSpike_alpha=5.30] Epoch 2 Test  | Loss: 1.0673 | Acc: 61.75%\n",
      "[SuperSpike_alpha=5.30] Epoch 2 Test  | Loss: 1.0673 | Acc: 61.75%\n",
      "[SuperSpike_alpha=5.30] Epoch 3 Train | Loss: 0.9924 | Acc: 65.17% | Time: 33.96s\n",
      "[SuperSpike_alpha=5.30] Epoch 3 Train | Loss: 0.9924 | Acc: 65.17% | Time: 33.96s\n",
      "[SuperSpike_alpha=5.30] Epoch 3 Test  | Loss: 0.9576 | Acc: 66.17%\n",
      "[SuperSpike_alpha=5.30] Epoch 3 Test  | Loss: 0.9576 | Acc: 66.17%\n",
      "[SuperSpike_alpha=5.30] Epoch 4 Train | Loss: 0.8793 | Acc: 69.03% | Time: 33.72s\n",
      "[SuperSpike_alpha=5.30] Epoch 4 Train | Loss: 0.8793 | Acc: 69.03% | Time: 33.72s\n",
      "[SuperSpike_alpha=5.30] Epoch 4 Test  | Loss: 0.9105 | Acc: 67.77%\n",
      "[SuperSpike_alpha=5.30] Epoch 4 Test  | Loss: 0.9105 | Acc: 67.77%\n",
      "[SuperSpike_alpha=5.30] Epoch 5 Train | Loss: 0.8045 | Acc: 71.98% | Time: 33.57s\n",
      "[SuperSpike_alpha=5.30] Epoch 5 Train | Loss: 0.8045 | Acc: 71.98% | Time: 33.57s\n",
      "[SuperSpike_alpha=5.30] Epoch 5 Test  | Loss: 0.8809 | Acc: 68.92%\n",
      "[SuperSpike_alpha=5.30] Epoch 5 Test  | Loss: 0.8809 | Acc: 68.92%\n",
      "[SuperSpike_alpha=5.30] Epoch 6 Train | Loss: 0.7392 | Acc: 74.08% | Time: 33.45s\n",
      "[SuperSpike_alpha=5.30] Epoch 6 Train | Loss: 0.7392 | Acc: 74.08% | Time: 33.45s\n",
      "[SuperSpike_alpha=5.30] Epoch 6 Test  | Loss: 0.8413 | Acc: 70.54%\n",
      "[SuperSpike_alpha=5.30] Epoch 6 Test  | Loss: 0.8413 | Acc: 70.54%\n",
      "[SuperSpike_alpha=5.30] Epoch 7 Train | Loss: 0.6875 | Acc: 76.09% | Time: 33.50s\n",
      "[SuperSpike_alpha=5.30] Epoch 7 Train | Loss: 0.6875 | Acc: 76.09% | Time: 33.50s\n",
      "[SuperSpike_alpha=5.30] Epoch 7 Test  | Loss: 0.8483 | Acc: 70.48%\n",
      "[SuperSpike_alpha=5.30] Epoch 7 Test  | Loss: 0.8483 | Acc: 70.48%\n",
      "[SuperSpike_alpha=5.30] Epoch 8 Train | Loss: 0.6423 | Acc: 77.57% | Time: 33.45s\n",
      "[SuperSpike_alpha=5.30] Epoch 8 Train | Loss: 0.6423 | Acc: 77.57% | Time: 33.45s\n",
      "[SuperSpike_alpha=5.30] Epoch 8 Test  | Loss: 0.8087 | Acc: 71.85%\n",
      "[SuperSpike_alpha=5.30] Epoch 8 Test  | Loss: 0.8087 | Acc: 71.85%\n",
      "[SuperSpike_alpha=5.30] Epoch 9 Train | Loss: 0.6005 | Acc: 79.28% | Time: 33.28s\n",
      "[SuperSpike_alpha=5.30] Epoch 9 Train | Loss: 0.6005 | Acc: 79.28% | Time: 33.28s\n",
      "[SuperSpike_alpha=5.30] Epoch 9 Test  | Loss: 0.8109 | Acc: 71.93%\n",
      "[SuperSpike_alpha=5.30] Epoch 9 Test  | Loss: 0.8109 | Acc: 71.93%\n",
      "[SuperSpike_alpha=5.30] Epoch 10 Train | Loss: 0.5611 | Acc: 80.43% | Time: 33.47s\n",
      "[SuperSpike_alpha=5.30] Epoch 10 Train | Loss: 0.5611 | Acc: 80.43% | Time: 33.47s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-08 14:23:37,442] Trial 6 finished with value: 71.93 and parameters: {'alpha': 5.2976422955482825}. Best is trial 0 with value: 72.29.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SuperSpike_alpha=5.30] Epoch 10 Test  | Loss: 0.8249 | Acc: 71.78%\n",
      "Initializing Network with Surrogate: SuperSpike\n",
      "[SuperSpike_alpha=2.08] Epoch 1 Train | Loss: 1.5897 | Acc: 42.56% | Time: 33.35s\n",
      "[SuperSpike_alpha=2.08] Epoch 1 Train | Loss: 1.5897 | Acc: 42.56% | Time: 33.35s\n",
      "[SuperSpike_alpha=2.08] Epoch 1 Test  | Loss: 1.2881 | Acc: 53.85%\n",
      "[SuperSpike_alpha=2.08] Epoch 1 Test  | Loss: 1.2881 | Acc: 53.85%\n",
      "[SuperSpike_alpha=2.08] Epoch 2 Train | Loss: 1.1969 | Acc: 57.50% | Time: 33.29s\n",
      "[SuperSpike_alpha=2.08] Epoch 2 Train | Loss: 1.1969 | Acc: 57.50% | Time: 33.29s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-08 14:24:51,749] Trial 7 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SuperSpike_alpha=2.08] Epoch 2 Test  | Loss: 1.1351 | Acc: 59.59%\n",
      "Initializing Network with Surrogate: SuperSpike\n",
      "[SuperSpike_alpha=11.36] Epoch 1 Train | Loss: 1.6061 | Acc: 41.74% | Time: 33.30s\n",
      "[SuperSpike_alpha=11.36] Epoch 1 Train | Loss: 1.6061 | Acc: 41.74% | Time: 33.30s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-08 14:25:28,877] Trial 8 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SuperSpike_alpha=11.36] Epoch 1 Test  | Loss: 1.2949 | Acc: 52.84%\n",
      "Initializing Network with Surrogate: SuperSpike\n",
      "[SuperSpike_alpha=2.92] Epoch 1 Train | Loss: 1.5639 | Acc: 43.50% | Time: 33.26s\n",
      "[SuperSpike_alpha=2.92] Epoch 1 Train | Loss: 1.5639 | Acc: 43.50% | Time: 33.26s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-08 14:26:06,130] Trial 9 pruned. \n",
      "[I 2025-12-08 14:26:06,132] A new study created in memory with name: no-name-4c90ebe9-824a-456e-b8fe-5635c0d90662\n",
      "[I 2025-12-08 14:26:06,132] A new study created in memory with name: no-name-4c90ebe9-824a-456e-b8fe-5635c0d90662\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SuperSpike_alpha=2.92] Epoch 1 Test  | Loss: 1.2895 | Acc: 53.73%\n",
      "[SuperSpike] 搜索结束。\n",
      "  Best Alpha: 6.5233\n",
      "  Best Acc: 72.29%\n",
      "\n",
      ">>> 开始搜索 Sigmoid 的最优 alpha (Trials: 10) ...\n",
      "Initializing Network with Surrogate: SigmoidDerivative\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to connect to the remote Jupyter Server 'https://8080-gpu-t4-s-1lqu30yjk5pvi-a.us-west4-1.prod.colab.dev/'. Verify the server is running and reachable."
     ]
    }
   ],
   "source": [
    "# ----------------------------------------\n",
    "# 6. 开始 Optuna 超参数搜索\n",
    "# ----------------------------------------\n",
    "print(f\"=== 开始对比实验 (总 Epochs: {EPOCHS}) ===\\n\")\n",
    "\n",
    "def run_optuna_search(surrogate_name, n_trials=10):\n",
    "    print(f\"\\n>>> 开始搜索 {surrogate_name} 的最优 alpha (Trials: {n_trials}) ...\")\n",
    "    \n",
    "    def objective(trial):\n",
    "        # 1. 采样超参数 alpha (搜索空间 0.5 - 20)\n",
    "        alpha = trial.suggest_float(\"alpha\", 0.5, 20.0)\n",
    "        \n",
    "        # 2. 构建模型和优化器\n",
    "        surr_func = get_surrogate_func(surrogate_name, alpha)\n",
    "        model = BasicCSNN(T=T, surrogate_function=surr_func).to(DEVICE)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "        \n",
    "        # 3. 训练循环\n",
    "        best_acc = 0.0\n",
    "        \n",
    "        for epoch in range(EPOCHS):\n",
    "            # 这里的 model_name 用于打印日志\n",
    "            trial_name = f\"{surrogate_name}_alpha={alpha:.2f}\"\n",
    "            \n",
    "            # 训练\n",
    "            train_loss, train_acc = train_epoch(model, optimizer, epoch, trial_name)\n",
    "            \n",
    "            # 测试\n",
    "            test_loss, test_acc = test_epoch(model, epoch, trial_name)\n",
    "            \n",
    "            if test_acc > best_acc:\n",
    "                best_acc = test_acc\n",
    "            \n",
    "            # Optuna Pruning: 如果当前 trial 表现不好，提前终止\n",
    "            trial.report(test_acc, epoch)\n",
    "            if trial.should_prune():\n",
    "                raise optuna.exceptions.TrialPruned()\n",
    "                \n",
    "        return best_acc\n",
    "\n",
    "    # 创建 Study\n",
    "    # direction=\"maximize\" 因为我们要最大化准确率\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "    \n",
    "    print(f\"[{surrogate_name}] 搜索结束。\")\n",
    "    print(f\"  Best Alpha: {study.best_params['alpha']:.4f}\")\n",
    "    print(f\"  Best Acc: {study.best_value:.2f}%\")\n",
    "    return study\n",
    "\n",
    "# 运行所有搜索\n",
    "studies = {}\n",
    "# 建议 trial 次数设为 10-20 次，根据计算资源调整\n",
    "N_TRIALS = 10 \n",
    "\n",
    "for name in surrogate_types:\n",
    "    studies[name] = run_optuna_search(name, n_trials=N_TRIALS)\n",
    "\n",
    "# ----------------------------------------\n",
    "# 7. 最终总结\n",
    "# ----------------------------------------\n",
    "print(f\"\\n=== 所有实验完成，最终结果汇总 ===\")\n",
    "for name, study in studies.items():\n",
    "    print(f\"模型: {name:<15} | 最优 Alpha: {study.best_params['alpha']:.4f} | 最佳 Test Acc: {study.best_value:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
