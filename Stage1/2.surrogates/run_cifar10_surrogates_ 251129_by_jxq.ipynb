{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4b160a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spikingjelly\n",
      "  Downloading spikingjelly-0.0.0.0.14-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from spikingjelly) (2.9.0+cu126)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from spikingjelly) (3.10.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from spikingjelly) (2.0.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from spikingjelly) (4.67.1)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from spikingjelly) (0.24.0+cu126)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from spikingjelly) (1.16.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->spikingjelly) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->spikingjelly) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->spikingjelly) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->spikingjelly) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->spikingjelly) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->spikingjelly) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->spikingjelly) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->spikingjelly) (2.9.0.post0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->spikingjelly) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->spikingjelly) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->spikingjelly) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->spikingjelly) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch->spikingjelly) (3.6)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->spikingjelly) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch->spikingjelly) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->spikingjelly) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->spikingjelly) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->spikingjelly) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->spikingjelly) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->spikingjelly) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->spikingjelly) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->spikingjelly) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->spikingjelly) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->spikingjelly) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->spikingjelly) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch->spikingjelly) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch->spikingjelly) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->spikingjelly) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->spikingjelly) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->spikingjelly) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch->spikingjelly) (3.5.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->spikingjelly) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->spikingjelly) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->spikingjelly) (3.0.3)\n",
      "Downloading spikingjelly-0.0.0.0.14-py3-none-any.whl (437 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m437.6/437.6 kB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: spikingjelly\n",
      "Successfully installed spikingjelly-0.0.0.0.14\n"
     ]
    }
   ],
   "source": [
    "# for colab use\n",
    "# %pip install spikingjelly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1b9769a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38838a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.0+cu126\n",
      "CUDA available: True\n",
      "Current device: Tesla T4\n",
      "Device count: 1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import time\n",
    "from spikingjelly.activation_based import neuron, functional, layer, surrogate\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Current device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Device count: {torch.cuda.device_count()}\") # 应该输出 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "759760e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 辅助函数：阶跃函数 ---\n",
    "@torch.jit.script\n",
    "def heaviside(x: torch.Tensor):\n",
    "    \"\"\"\n",
    "    前向传播用的阶跃函数：x >= 0 时输出 1，否则输出 0\n",
    "    \"\"\"\n",
    "    return (x >= 0).float()\n",
    "\n",
    "# =========================================================\n",
    "# 1. SuperSpike 实现\n",
    "# 公式: h(x) = 1 / (beta * |x| + 1)^2\n",
    "# =========================================================\n",
    "class SuperSpikeFunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, alpha):\n",
    "        if x.requires_grad:\n",
    "            ctx.save_for_backward(x)\n",
    "            ctx.alpha = alpha\n",
    "        return heaviside(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        x, = ctx.saved_tensors\n",
    "        alpha = ctx.alpha\n",
    "        \n",
    "        # 实现图片中的公式: 1 / (beta * |x| + 1)^2\n",
    "        denom = (alpha * x.abs() + 1.0)\n",
    "        grad_x = grad_output * (1.0 / (denom * denom))\n",
    "        \n",
    "        return grad_x, None\n",
    "\n",
    "class SuperSpike(nn.Module):\n",
    "    def __init__(self, alpha=100.0, spiking=True):\n",
    "        \"\"\"\n",
    "        SuperSpike 替代梯度\n",
    "        :param alpha: 对应公式中的 beta，控制梯度的陡峭程度\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.spiking = spiking\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.spiking:\n",
    "            return SuperSpikeFunction.apply(x, self.alpha)\n",
    "        else:\n",
    "            return heaviside(x)\n",
    "\n",
    "# =========================================================\n",
    "# 2. Sigmoid' (Image Version) 实现\n",
    "# 公式: h(x) = s(x)(1 - s(x)), 其中 s(x) = sigmoid(beta * x)\n",
    "# =========================================================\n",
    "class SigmoidDerivativeFunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, alpha):\n",
    "        if x.requires_grad:\n",
    "            ctx.save_for_backward(x)\n",
    "            ctx.alpha = alpha\n",
    "        return heaviside(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        x, = ctx.saved_tensors\n",
    "        alpha = ctx.alpha\n",
    "        \n",
    "        # 计算 s(x) = sigmoid(beta * x)\n",
    "        sigmoid_x = torch.sigmoid(alpha * x)\n",
    "        \n",
    "        # 实现: h(x) = s(x) * (1 - s(x))\n",
    "        grad_x = grad_output * sigmoid_x * (1.0 - sigmoid_x)\n",
    "        \n",
    "        return grad_x, None\n",
    "\n",
    "class SigmoidDerivative(nn.Module):\n",
    "    def __init__(self, alpha=4.0, spiking=True):\n",
    "        \"\"\"\n",
    "        Sigmoid' 替代梯度 (图片版本)\n",
    "        :param alpha: 对应公式中的 beta\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.spiking = spiking\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.spiking:\n",
    "            return SigmoidDerivativeFunction.apply(x, self.alpha)\n",
    "        return heaviside(x)\n",
    "\n",
    "# =========================================================\n",
    "# 3. Esser et al. 实现\n",
    "# 公式: h(x) = max(0, 1.0 - beta * |x|)\n",
    "# =========================================================\n",
    "class EsserFunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, alpha):\n",
    "        if x.requires_grad:\n",
    "            ctx.save_for_backward(x)\n",
    "            ctx.alpha = alpha\n",
    "        return heaviside(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        x, = ctx.saved_tensors\n",
    "        alpha = ctx.alpha\n",
    "        \n",
    "        # 实现图片公式: max(0, 1.0 - beta * |x|)\n",
    "        grad_x = grad_output * torch.clamp(1.0 - alpha * x.abs(), min=0.0)\n",
    "        \n",
    "        return grad_x, None\n",
    "\n",
    "class Esser(nn.Module):\n",
    "    def __init__(self, alpha=1.0, spiking=True):\n",
    "        \"\"\"\n",
    "        Esser et al. 替代梯度\n",
    "        :param alpha: 对应公式中的 beta，通常设为 1.0 或更大\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.spiking = spiking\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.spiking:\n",
    "            return EsserFunction.apply(x, self.alpha)\n",
    "        return heaviside(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "002a6234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 实验设置 ---\n",
      "设备 (DEVICE): cuda:0\n",
      "仿真时长 (T): 8\n",
      "批大小 (BATCH_SIZE): 64\n",
      "训练轮数 (EPOCHS): 10\n",
      "------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------\n",
    "# 1. 定义超参数和设置\n",
    "# ----------------------------------------\n",
    "\n",
    "T = 8             # 仿真总时长 (SNN 的关键参数)\n",
    "BATCH_SIZE = 64   # 批处理大小\n",
    "EPOCHS = 10       # 训练轮数 (为快速演示，设置较小)\n",
    "LR = 1e-3         # 学习率\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "BETA = 10.0       # 替代梯度中的超参数, 论文中规定值\n",
    "\n",
    "print(f\"--- 实验设置 ---\")\n",
    "print(f\"设备 (DEVICE): {DEVICE}\")\n",
    "print(f\"仿真时长 (T): {T}\")\n",
    "print(f\"批大小 (BATCH_SIZE): {BATCH_SIZE}\")\n",
    "print(f\"训练轮数 (EPOCHS): {EPOCHS}\")\n",
    "print(f\"------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b391e614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在加载 CIFAR10 数据集...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170M/170M [00:14<00:00, 11.9MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据集加载完毕。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------\n",
    "# 2. 加载和预处理 CIFAR10 数据集\n",
    "# ----------------------------------------\n",
    "print(\"正在加载 CIFAR10 数据集...\")\n",
    "# CIFAR10 图像的均值和标准差 (用于归一化)\n",
    "cifar_mean = (0.4914, 0.4822, 0.4465)\n",
    "cifar_std = (0.2023, 0.1994, 0.2010)\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(), # 简单数据增强：随机翻转\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(cifar_mean, cifar_std)\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(cifar_mean, cifar_std)\n",
    "])\n",
    "\n",
    "# 加载数据\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "print(\"数据集加载完毕。\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86696b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------\n",
    "# 3. 定义基础的卷积 SNN 模型\n",
    "# ----------------------------------------\n",
    "# 使用 nn.Sequential 快速搭建一个简单的 CNN 结构\n",
    "# 关键在于在激活函数的位置换上 SNN 的脉冲神经元\n",
    "\n",
    "class BasicCSNN(nn.Module):\n",
    "    # 增加 surrogate_function 参数\n",
    "    def __init__(self, T, surrogate_function=surrogate.Sigmoid()):\n",
    "        super().__init__()\n",
    "        self.T = T  # 保存仿真时长\n",
    "        print(f\"Initializing Network with Surrogate: {surrogate_function.__class__.__name__}\")\n",
    "\n",
    "        # 定义网络结构\n",
    "        # 结构：[卷积 -> 脉冲 -> 池化] x 2 -> [展平 -> 全连接 -> 脉冲] -> [全连接]\n",
    "        self.net = nn.Sequential(\n",
    "            # 块 1\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            # --- 核心：使用 LIF 神经元 ---\n",
    "            # 激活驱动:LIFNode 在前向传播时模拟 LIF 神经元动力学，在反向传播时，SpikingJelly 会自动使用“替代梯度”进行计算。\n",
    "            neuron.LIFNode(surrogate_function=surrogate_function),\n",
    "            nn.MaxPool2d(2),  # 32x32 -> 16x16\n",
    "\n",
    "            # 块 2\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            neuron.LIFNode(surrogate_function=surrogate_function),\n",
    "            nn.MaxPool2d(2),  # 16x16 -> 8x8\n",
    "\n",
    "            # 展平\n",
    "            nn.Flatten(),\n",
    "\n",
    "            # 全连接层 1\n",
    "            nn.Linear(64 * 8 * 8, 128), # 64 * 8 * 8 = 4096\n",
    "            neuron.LIFNode(surrogate_function=surrogate_function),\n",
    "\n",
    "            # 输出层 (全连接层 2)\n",
    "            # 输出层通常不使用脉冲神经元，而是直接输出膜电位或累积电流\n",
    "            # 这样可以方便地与交叉熵损失配合使用\n",
    "            nn.Linear(128, 10) # 10个类别\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # --- SNN 算法思路的核心 ---\n",
    "        # SNN 神经元是有状态的（例如膜电位 V），在处理一个新样本前必须重置\n",
    "        # 1. 重置网络中所有神经元的状态\n",
    "        functional.reset_net(self)\n",
    "\n",
    "        # 准备一个列表来收集 T 个时间步的输出\n",
    "        # (T, N, C)，T=时间步, N=BatchSize, C=类别数\n",
    "        outputs_over_time = []\n",
    "\n",
    "        # 2. SNN 的时间步循环\n",
    "        # 对于静态图像 (如CIFAR10)，我们在 T 个时间步内输入 *相同* 的图像 x\n",
    "        # 神经元会在这 T 步内不断累积输入并发放脉冲\n",
    "        for t in range(self.T):\n",
    "            # 运行一步前向传播\n",
    "            out_t = self.net(x)\n",
    "            outputs_over_time.append(out_t)\n",
    "\n",
    "        # 3. 聚合 T 个时间步的输出\n",
    "        # (T, N, 10) -> (T, N, 10)\n",
    "        outputs_stack = torch.stack(outputs_over_time)\n",
    "        \n",
    "        # 4. 解码：计算 T 步内的平均输出\n",
    "        # (T, N, 10) -> (N, 10)\n",
    "        # 我们取所有时间步输出的平均值，作为最终的分类 \"logits\"\n",
    "        # 这是一种常见的 SNN 解码方式（Rate Coding / Mean Output）\n",
    "        return outputs_stack.mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8cf29bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Network with Surrogate: SuperSpike\n",
      "Initializing Network with Surrogate: SigmoidDerivative\n",
      "Initializing Network with Surrogate: Esser\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------\n",
    "# 4. 初始化模型、损失函数和优化器\n",
    "# ----------------------------------------\n",
    "# 1. 准备实验配置\n",
    "# 我们用一个列表来存储所有要对比的实验对象\n",
    "experiments = []\n",
    "\n",
    "# 定义要对比的替代梯度\n",
    "surrogates_config = [\n",
    "    (\"SuperSpike\", SuperSpike(alpha=10.0)),\n",
    "    (\"Sigmoid\",    SigmoidDerivative(alpha=4.0)),\n",
    "    (\"Esser\",      Esser(alpha=1.0))\n",
    "]\n",
    "\n",
    "# 2. 初始化所有模型和优化器，填入 experiments 列表\n",
    "for name, surr_func in surrogates_config:\n",
    "    # 实例化模型\n",
    "    net = BasicCSNN(T=T, surrogate_function=surr_func).to(DEVICE)\n",
    "    \n",
    "    # 实例化对应的优化器 (每个模型有自己独立的参数)\n",
    "    opt = optim.Adam(net.parameters(), lr=LR)\n",
    "    \n",
    "    # 将它们打包存起来，顺便准备好存结果的 list\n",
    "    experiments.append({\n",
    "        \"name\": name,\n",
    "        \"model\": net,\n",
    "        \"optimizer\": opt,\n",
    "        \"train_acc_history\": [],\n",
    "        \"test_acc_history\": []\n",
    "    })\n",
    "\n",
    "# 使用标准的交叉熵损失函数\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5d6e739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------\n",
    "# 5. 训练和评估循环\n",
    "# ----------------------------------------\n",
    "\n",
    "# --- 训练函数 (Train Loop) ---\n",
    "def train_epoch(model, optimizer, epoch, model_name):\n",
    "    model.train()  # 设置为训练模式\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs) # 这里的 model 是参数传进来的\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        # 统计损失和准确率\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "\n",
    "    end_time = time.time()\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    acc = 100. * correct / total\n",
    "    print(f\"[{model_name}] Epoch {epoch+1} Train | Loss: {avg_loss:.4f} | Acc: {acc:.2f}% | Time: {end_time - start_time:.2f}s\")\n",
    "    return avg_loss, acc\n",
    "\n",
    "# --- 评估函数 (Eval Loop) ---\n",
    "def test_epoch(model, epoch, model_name):\n",
    "    model.eval()  # 设置为评估模式\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # 评估时不需要计算梯度\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "\n",
    "            # 前向传播\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # 计算损失\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # 统计准确率\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            \n",
    "    acc = 100. * correct / total\n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    print(f\"[{model_name}] Epoch {epoch+1} Test  | Loss: {avg_loss:.4f} | Acc: {acc:.2f}%\")\n",
    "    return avg_loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f8600fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 开始对比实验 (总 Epochs: 10) ===\n",
      "\n",
      " >> 正在训练模型: [SuperSpike] ...\n",
      "[SuperSpike] Epoch 1 Train | Loss: 1.5635 | Acc: 43.80% | Time: 34.61s\n",
      "[SuperSpike] Epoch 1 Test  | Loss: 1.2612 | Acc: 54.59%\n",
      "[SuperSpike] Epoch 2 Train | Loss: 1.1668 | Acc: 58.22% | Time: 32.59s\n",
      "[SuperSpike] Epoch 2 Test  | Loss: 1.0899 | Acc: 61.27%\n",
      "[SuperSpike] Epoch 3 Train | Loss: 1.0219 | Acc: 63.90% | Time: 32.15s\n",
      "[SuperSpike] Epoch 3 Test  | Loss: 0.9941 | Acc: 64.91%\n",
      "[SuperSpike] Epoch 4 Train | Loss: 0.9180 | Acc: 67.59% | Time: 32.18s\n",
      "[SuperSpike] Epoch 4 Test  | Loss: 0.9338 | Acc: 67.45%\n",
      "[SuperSpike] Epoch 5 Train | Loss: 0.8510 | Acc: 70.14% | Time: 32.18s\n",
      "[SuperSpike] Epoch 5 Test  | Loss: 0.9194 | Acc: 67.77%\n",
      "[SuperSpike] Epoch 6 Train | Loss: 0.7938 | Acc: 72.20% | Time: 31.35s\n",
      "[SuperSpike] Epoch 6 Test  | Loss: 0.8818 | Acc: 69.11%\n",
      "[SuperSpike] Epoch 7 Train | Loss: 0.7415 | Acc: 74.05% | Time: 31.30s\n",
      "[SuperSpike] Epoch 7 Test  | Loss: 0.8632 | Acc: 70.06%\n",
      "[SuperSpike] Epoch 8 Train | Loss: 0.7023 | Acc: 75.36% | Time: 32.04s\n",
      "[SuperSpike] Epoch 8 Test  | Loss: 0.8672 | Acc: 70.13%\n",
      "[SuperSpike] Epoch 9 Train | Loss: 0.6636 | Acc: 76.77% | Time: 31.97s\n",
      "[SuperSpike] Epoch 9 Test  | Loss: 0.8325 | Acc: 71.19%\n",
      "[SuperSpike] Epoch 10 Train | Loss: 0.6227 | Acc: 78.60% | Time: 32.11s\n",
      "[SuperSpike] Epoch 10 Test  | Loss: 0.8245 | Acc: 71.83%\n",
      " << 模型 [SuperSpike] 训练结束。最佳测试准确率: 71.83%\n",
      "\n",
      "------------------------------------------------------------\n",
      " >> 正在训练模型: [Sigmoid] ...\n",
      "[Sigmoid] Epoch 1 Train | Loss: 1.5410 | Acc: 44.48% | Time: 31.02s\n",
      "[Sigmoid] Epoch 1 Test  | Loss: 1.2744 | Acc: 54.28%\n",
      "[Sigmoid] Epoch 2 Train | Loss: 1.1733 | Acc: 58.31% | Time: 30.46s\n",
      "[Sigmoid] Epoch 2 Test  | Loss: 1.0884 | Acc: 61.71%\n",
      "[Sigmoid] Epoch 3 Train | Loss: 1.0139 | Acc: 64.33% | Time: 30.36s\n",
      "[Sigmoid] Epoch 3 Test  | Loss: 0.9975 | Acc: 64.58%\n",
      "[Sigmoid] Epoch 4 Train | Loss: 0.9057 | Acc: 68.20% | Time: 31.31s\n",
      "[Sigmoid] Epoch 4 Test  | Loss: 0.9149 | Acc: 68.22%\n",
      "[Sigmoid] Epoch 5 Train | Loss: 0.8247 | Acc: 70.98% | Time: 30.68s\n",
      "[Sigmoid] Epoch 5 Test  | Loss: 0.8978 | Acc: 68.47%\n",
      "[Sigmoid] Epoch 6 Train | Loss: 0.7696 | Acc: 73.11% | Time: 30.56s\n",
      "[Sigmoid] Epoch 6 Test  | Loss: 0.8610 | Acc: 69.67%\n",
      "[Sigmoid] Epoch 7 Train | Loss: 0.7127 | Acc: 75.07% | Time: 30.06s\n",
      "[Sigmoid] Epoch 7 Test  | Loss: 0.8569 | Acc: 70.21%\n",
      "[Sigmoid] Epoch 8 Train | Loss: 0.6703 | Acc: 76.76% | Time: 30.54s\n",
      "[Sigmoid] Epoch 8 Test  | Loss: 0.8373 | Acc: 70.40%\n",
      "[Sigmoid] Epoch 9 Train | Loss: 0.6259 | Acc: 78.34% | Time: 30.91s\n",
      "[Sigmoid] Epoch 9 Test  | Loss: 0.8287 | Acc: 71.26%\n",
      "[Sigmoid] Epoch 10 Train | Loss: 0.5885 | Acc: 79.65% | Time: 31.52s\n",
      "[Sigmoid] Epoch 10 Test  | Loss: 0.8134 | Acc: 71.98%\n",
      " << 模型 [Sigmoid] 训练结束。最佳测试准确率: 71.98%\n",
      "\n",
      "------------------------------------------------------------\n",
      " >> 正在训练模型: [Esser] ...\n",
      "[Esser] Epoch 1 Train | Loss: 1.5960 | Acc: 42.36% | Time: 29.79s\n",
      "[Esser] Epoch 1 Test  | Loss: 1.2933 | Acc: 53.17%\n",
      "[Esser] Epoch 2 Train | Loss: 1.2174 | Acc: 56.64% | Time: 30.08s\n",
      "[Esser] Epoch 2 Test  | Loss: 1.1504 | Acc: 59.43%\n",
      "[Esser] Epoch 3 Train | Loss: 1.0579 | Acc: 62.52% | Time: 30.68s\n",
      "[Esser] Epoch 3 Test  | Loss: 1.0417 | Acc: 63.10%\n",
      "[Esser] Epoch 4 Train | Loss: 0.9548 | Acc: 66.26% | Time: 30.74s\n",
      "[Esser] Epoch 4 Test  | Loss: 0.9602 | Acc: 65.83%\n",
      "[Esser] Epoch 5 Train | Loss: 0.8782 | Acc: 69.32% | Time: 30.19s\n",
      "[Esser] Epoch 5 Test  | Loss: 0.9279 | Acc: 67.34%\n",
      "[Esser] Epoch 6 Train | Loss: 0.8156 | Acc: 71.28% | Time: 31.12s\n",
      "[Esser] Epoch 6 Test  | Loss: 0.9195 | Acc: 67.86%\n",
      "[Esser] Epoch 7 Train | Loss: 0.7690 | Acc: 73.08% | Time: 30.69s\n",
      "[Esser] Epoch 7 Test  | Loss: 0.9114 | Acc: 68.05%\n",
      "[Esser] Epoch 8 Train | Loss: 0.7272 | Acc: 74.47% | Time: 30.76s\n",
      "[Esser] Epoch 8 Test  | Loss: 0.8880 | Acc: 68.69%\n",
      "[Esser] Epoch 9 Train | Loss: 0.6857 | Acc: 76.12% | Time: 30.02s\n",
      "[Esser] Epoch 9 Test  | Loss: 0.8539 | Acc: 70.45%\n",
      "[Esser] Epoch 10 Train | Loss: 0.6514 | Acc: 77.41% | Time: 29.88s\n",
      "[Esser] Epoch 10 Test  | Loss: 0.8724 | Acc: 70.35%\n",
      " << 模型 [Esser] 训练结束。最佳测试准确率: 70.45%\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "=== 所有实验完成，最终结果汇总 ===\n",
      "模型: SuperSpike      | 最佳 Test Acc: 71.83%\n",
      "模型: Sigmoid         | 最佳 Test Acc: 71.98%\n",
      "模型: Esser           | 最佳 Test Acc: 70.45%\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------\n",
    "# 6. 开始批量训练 (Batch Training Loop)\n",
    "# ----------------------------------------\n",
    "print(f\"=== 开始对比实验 (总 Epochs: {EPOCHS}) ===\\n\")\n",
    "\n",
    "# 遍历我们在 experiment 列表中定义的每一个实验对象\n",
    "for exp in experiments:\n",
    "    # 1. 解包当前实验的变量\n",
    "    model_name = exp[\"name\"]\n",
    "    model = exp[\"model\"]\n",
    "    optimizer = exp[\"optimizer\"]\n",
    "    \n",
    "    # 每个模型有自己独立的最佳准确率记录\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    print(f\" >> 正在训练模型: [{model_name}] ...\")\n",
    "    \n",
    "    # 2. 针对当前模型进行 Epoch 循环\n",
    "    for epoch in range(EPOCHS):\n",
    "        # 调用修改后的训练函数 (传入当前模型和优化器)\n",
    "        # 注意：这里我们接收返回值，以便记录历史\n",
    "        _, train_acc = train_epoch(model, optimizer, epoch, model_name)\n",
    "        \n",
    "        # 调用修改后的测试函数\n",
    "        _, test_acc = test_epoch(model, epoch, model_name)\n",
    "        \n",
    "        # 3. 记录数据 (用于后续画图)\n",
    "        exp[\"train_acc_history\"].append(train_acc)\n",
    "        exp[\"test_acc_history\"].append(test_acc)\n",
    "        \n",
    "        # 4. 更新当前模型的最佳记录\n",
    "        if test_acc > best_acc:\n",
    "            best_acc = test_acc\n",
    "            # 如果你想保存表现最好的模型权重\n",
    "            # torch.save(model.state_dict(), f'{model_name}_best.pth')\n",
    "\n",
    "    # 将最佳结果存入字典，方便最后总结\n",
    "    exp[\"best_acc\"] = best_acc\n",
    "    \n",
    "    print(f\" << 模型 [{model_name}] 训练结束。最佳测试准确率: {best_acc:.2f}%\\n\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "# ----------------------------------------\n",
    "# 7. 最终总结\n",
    "# ----------------------------------------\n",
    "print(f\"\\n=== 所有实验完成，最终结果汇总 ===\")\n",
    "for exp in experiments:\n",
    "    print(f\"模型: {exp['name']:<15} | 最佳 Test Acc: {exp['best_acc']:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
